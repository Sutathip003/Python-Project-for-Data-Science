# from bs4 import BeautifulSoup
# import requests
# URL = "http://www.example.com"
# page = requests.get(URL)
# soup = BeautifulSoup(page.content, "html.parser")


# import scrapy
# class QuotesSpider(scrapy.Spider):
#     name = "quotes"
#     start_urls = ['http://quotes.toscrape.com/tag/humor/',]
#     def parse(self, response):
#         for quote in response.css('div.quote'):
#             yield {'quote': quote.css('span.text::text').get()}


# from selenium import webdriver
# driver = webdriver.Firefox()
# driver.get("http://www.example.com")





# Install libraries for web scraping and data analysis
!pip install bs4
!pip install lxml
!pip install html5lib
!pip install pandas


# suppress all warnings
import warnings
warnings.simplefilter("ignore")


from bs4 import BeautifulSoup # this module helps in web scrapping.
import requests # this module helps us to download a web page


%%html
<!DOCTYPE  html>
<html>
    
<head>
    <title>Page Title</title>
</head>    
    <body>
        <h3><b id= "boldest">Lebron James</b></h3>
        <p>Salary: $ 92,000,000</p>
        <h3>Stephen Curry</h3>
        <p>Salary: $ 85,000,000</p>
        <h3><Kevin Durant</h3>
        <p>Salary: $ 73,000,000</p>
    </body>    
</html>

    


# We can store it as a string in the variable HTML
html = """<!DOCTYPE  html><html><head><title>Page Title</title></head><body><h3><b id= "boldest">Lebron James</b></h3><p>Salary: $ 92,000,000</p><h3>Stephen Curry</h3><p>Salary: $ 85,000,000</p><h3><Kevin Durant</h3><p>Salary: $ 73,000,000</p></body></html>"""


soup = BeautifulSoup(html,"html.parser")


print(soup.prettify())


# check title tag
tag_object = soup.title
print("tag_title: ", tag_object)


# see tag type
print("tag object type: ", type(tag_object))


tag_object = soup.h3
tag_object





t_chil = tag_object.b
t_chil


# access parants
parentTag = t_chil.parent
parentTag


tag_object.parent


sib1 = tag_object.next_sibling
sib1


sib2 = sib1.next_sibling
sib2





t_chil["id"]


t_chil.attrs


# get id
t_chil.get("id")


## Navigable String
t_str = t_chil.string
t_str


type(t_str)


# convert NavigableString to Python or unicode string
pythonStr = str(t_str)
pythonStr





%%html
<h3>Rocket Launch </h3>
<p>
<table class='rocket'>
<tr>
 <td id='flight' >Flight No</td>
 <td>Launch site</td>
 <td>Payload mass</td>
</tr>

<tr>
 <td>1</td>
 <td><a href ='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>
 <td>300 kg</td>
</tr>
 
<tr>
 <td>2</td>
 <td><a href ='https://en.wikipedia.org/wiki/Texas'>Texas</a></td>
 <td>94 kg</td>
</tr>

<tr>
 <td>3</td>
 <td><a href ='https://en.wikipedia.org/wiki/Florida'>Florida</a></td>
 <td>80 kg</td>
</tr>
</table>
</p>
<p>


# Add html to string
table = "<table><tr><td id='flight' >Flight No</td><td>Launch site</td><td>Payload mass</td></tr><tr><td>1</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a></td><td>300 kg</td></tr><tr><td>2</td><td><a href='https://en.wikipedia.org/wiki/Texas'>Texas</a></td><td>94 kg</td></tr><tr><td>3</td><td><a href='https://en.wikipedia.org/wiki/Florida'>Florida</a> </td><td>80 kg</td></tr></table>"

# create BeautifulSoup object
table_bs = BeautifulSoup(table,"html.parser")


# usr findall looks a tag's descendants
tb_rows = table_bs.find_all("tr")
tb_rows


first_row = tb_rows[0]
print("This is first row: ",first_row)

first_row.td


for i, row in enumerate(tb_rows):
    print("row: ", i, "is ", row)


for i, row in enumerate(tb_rows):
    print("row ", i)
    cells= row.find_all("td")
    for j, cell in enumerate(cells):
        print("column ", j, "cell ",cell)


# use list 
list_input = table_bs.find_all(name=["tr", "td"])
print(list_input)





table_bs.find_all(id="flight")
table_bs


# find link (href = hyperlink referance use to define destination of a link)
table_bs.find_all(href=True)


soup.find_all(id="boldest")





table_bs.find(table)

# filter on the class attribute
table_bs.find("table")





# Download content of the web page
import requests

url = "https://web.archive.org/web/20230224123642/https://www.ibm.com/us-en/"

data = requests.get(url).text

# create BeautifulSoup object
soup = BeautifulSoup(data, "html.parser")

# Scrap all links
for link in soup.find_all("a", href=True):
    print(link.get("href"))





for link in soup.find_all("img"):
    print(link)
    print(link.get("src"))






url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DA0321EN-SkillsNetwork/labs/datasets/HTMLColorCodes.html"

# get contant of web page
data = requests.get(url).text

# create soup object
soup = BeautifulSoup(data,"html.parser")

# find html table in the web page
table = soup.find("table")

# Get all rows of the table
for row in table.find_all("tr"):
    # Get colums in each row
    cols = row.find_all("td")
    color_name = cols[2].string
    color_code = cols[3].string
    print("{}----->{}".format(color_name, color_code))






import pandas as pd

url = url = "https://en.wikipedia.org/wiki/World_population"

data = requests.get(url).text

soup = BeautifulSoup(data, "html.parser")

tables  = soup.find_all("table")

print(len(tables))

for index, table in enumerate(tables):
    if ("10 most densely populated countries" in str(table)):
        table_index = index

print(table_index)



print(tables[table_index].prettify())


# create dataframe

#Crate colums head
poppu_data = pd.DataFrame(columns=["Rank","Country","Population", "Area", "Density"])

# screp data each row
for row in tables[table_index].tbody.find_all("tr"):
    col = row.find_all("td")
    if col:
        rank = col[0].text.strip()
        country = col[1].text.strip()
        population = col[2].text.strip()
        area = col[3].text.strip()
        density = col[4].text.strip()

        # create temporary dataframe for the new row
        NewRow = pd.DataFrame([{"Rank":rank,"Country":country,"Population":population, "Area":area, "Density":density}])

        # Use concat
        poppu_data = pd.concat([poppu_data, NewRow], ignore_index=True)

poppu_data






import bs4
print(bs4.__version__)


pd.read_html(str(tables[5]), flavor='bs4') # หากลองใช้ pd.read_html() แล้วเจอปัญหา ดึงข้อมูลไม่ได้ หรือดึงมาผิดเพี้ยน การเปลี่ยนมาใช้ flavor='bs4'


#The function read_html always returns a list of DataFrames

population_data_read_html = pd.read_html(str(tables[5]), flavor='bs4')[0]

population_data_read_html





dataframe_list = pd.read_html(url, flavor='bs4')

len(dataframe_list)

dataframe_list[5]


#use the match parameter to select the specific table 
pd.read_html(url, match="10 most densely populated countries", flavor='bs4')[0]



